\section{Conclusion}
The numerical stability of the GLOW architecture depends on the requirement that affine coupling layers compute scale coefficients $s$ for which each component is bounded away from zero. In the original GLOW architecture, this requirement is not enforced, so that affine coupling may cause large scale numerical problems while behaving as intended at a small scale. We examine multiple methods to enforce this requirement and find that the Biased Sigmoid and Bounded Net methods provide simple "plug-and-play" options which do not inhibit GLOW training.  This is a direct improvement over the Additive Coupling method considered in \cite{kingma2018glow}. Additionally, a third method using random channelwise permutations is shown to amplify the problem, a surprising result given that 1x1 invertible convolutions are explicitly designed to be able to learn arbitrary channelwise permutations. This gives evidence that the role of noise in image synthesis using the GLOW architecture is not fully understood, making it an interesting direction for future work.